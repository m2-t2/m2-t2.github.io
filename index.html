<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place</title>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120436611-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-120436611-3');
    </script>
    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">google.load("jquery", "1.3.2");</script>
    <script src="video.js"></script>

    <link rel="StyleSheet" href="style.css" type="text/css"/>
  </head>

  <body>
    <br>
    <center>
      <h1 class="title">M2T2: <b>M</b>ulti-<b>T</b>ask <b>M</b>asked-<b>T</b>ransformer</h1>
      <h1 class="subtitle">for Object-centric Pick and Place</h1>
    </center>

    <div class="container grey_container">
      <p class="text">
        Dear reviewers, to provide context to the rebuttal, please checkout the updated <a href="#network">network architecture figures</a>, <a href="#comparison">comparison against single-task models</a> and <a href="#video">real-world evaluation videos</a>, especially scene 5, 6, 7 for long-horizon tasks.
      </p>
    </div>

    <div class="container">
      <!-- <div class="container widget_container">
        <video controls loop autoplay muted playsinline class="on">
          <source src="videos/scene1_ours.mp4" type="video/mp4"/>
        </video>
      </div> -->
      <div class="container">
        <p class="text">
          M2T2 is a unified transformer model for picking and placing. From a raw 3D point cloud, M2T2 predicts 6-DoF grasps for each object on the table and orientation-aware placements for the object holded by the robot.
        </p>
        <img src="images/teaser.png"></img>
      </div>
    </div>

    <div class="container grey_container" id="video">
      <center><h2 class="sectitle">Real-world Pick-and-place</h2></center>
      <p class="text">
        M2T2 achieves zero-shot Sim2Real transfer for picking and placing unknown objects, outperforming a baseline system consisting of state-of-the-art task-specific methods by 19% in success rate. 
      </p>
      <div class="container widget_container">
        <div class="buttons">
          <button id="scene0" value="scene1" class="on">Scene 1</button>
          <button id="scene1" value="scene2">Scene 2</button>
          <button id="scene2" value="scene3">Scene 3</button>
          <button id="scene3" value="scene4">Scene 4</button>
          <button id="scene4" value="scene5">Scene 5</button>
          <button id="scene5" value="scene6">Scene 6</button>
          <button id="scene6" value="scene7">Scene 7</button>
        </div>
        <div class="buttons">
          <button id="btn0" value="ours" class="on">M2T2</button>
          <button id="btn1" value="baseline">Contact-GraspNet + CabiNet</button>
        </div>
        <div class="container video_container">
          <video id="vid0" controls loop muted playsinline class="on">
            <source src="videos/scene1_ours.mp4" type="video/mp4"/>
          </video>
          <video id="vid1" controls loop muted playsinline>
            <source src="videos/scene1_baseline.mp4" type="video/mp4"/>
          </video>
        </div>
      </div>
    </div>

    <div class="container" id="network">
      <center><h2 class="sectitle">Network Architecture</h2></center>
      <p class="text">
        M2T2 uses cross-attention between learned embeddings and multi-scale point cloud features to produce per-point contact masks, indicating where to make contact for picking and placing actions. Our general pick-and-place network produces G object-specific grasping masks, 1 for each graspable object in the scene, and P orientation-specific placement masks, 1 for each discretized planar rotation. 6-DoF gripper poses are then reconstructed using the contact masks and the point cloud.
      </p>
      <div class="container">
        <img src="images/network.png"></img>
      </div>
      <p class="text">
        M2T2 can also take other conditional inputs (e.g. language goal) to predict task-specific grasps/placements. Below is the architecture for M2T2 trained on RLBench, which is conditioned on language tokens embedded by a pretrained CLIP model.
      </p>
      <div class="container">
        <img src="images/rlbench_network.png"></img>
      </div>
    </div>

    <div class="container grey_container" id="comparison">
      <center><h2 class="sectitle">Comparison Against Single-task Models</h2></center>
      <p class="text">
        We have trained M2T2 to only perform a single task: grasping or placing. Although these task-specialized models outperform the baselines, they are still worse than our multi-task model. This shows that it is important to formulate both picking and placing under the same framework. 
      </p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr>
      <td width=50%" align="left" valign="middle">
        <img src="images/multi-task_vs_pick.png"></img>
      </td>
      <td width=50%" align="right" valign="middle">
        <img src="images/multi-task_vs_place.png"></img>
      </td>
      </tr>
      </table>
    </div>

  </body>
</html>
